# -*- coding: utf-8 -*-
"""LLM-as-a-Judge.ipynb

Automatically generated by Colab.
"""

from google.colab import userdata

# Instalar bibliotecas
# %pip install pandas openai python-dotenv

import pandas as pd
import openai
import os


openai.api_key = userdata.get('OPEN_AI_KEY')

if not openai.api_key:
    raise ValueError("A chave da API da OpenAI não foi encontrada. Configure OPENAI_API_KEY no seu arquivo .env")

print("Configuração inicial concluída!")

dimensoes = {
  "clareza_enunciado": "O enunciado da questão é formulado de forma clara e não ambígua, facilitando a compreensão do que está sendo perguntado?",
  "concisao_enunciado": "O enunciado da questão está livre de informações desnecessárias para respondê-la?",
  "clareza_alternativas": "Todas as alternativas são formuladas de forma clara e não ambígua?",
  "plausibilidade_distratores": "As alternativas incorretas (distratores) são plausíveis para estudantes com conhecimento parcial, mas claramente incorretas para estudantes bem preparados?",
  "consistencia_gramatical": "Todas as alternativas são gramaticalmente consistentes com o enunciado?",
  "uniformidade_alternativas": "Todas as alternativas são similares em tamanho e estrutura, evitando pistas que possam levar à resposta correta?",
  "resposta_unica": "Existe exatamente uma, e apenas uma, alternativa que seja claramente a melhor ou a correta dentre as fornecidas?",
  "ausencia_pistas": "A resposta correta é identificada pelo conhecimento no tópico, em vez da repetição de palavras do enunciado que não estão presentes nos distratores?",
  "relacao_topico": "A questão está relacionada ao tópico fornecido no prompt?",
  "correcao_gramatical_geral": "A questão inteira (enunciado e alternativas) está gramaticalmente correta e livre de erros de digitação?",
  "complexidade": "A questão exige uma análise mais aprofundada, além da simples memorização ou recordação de fatos?",
  "alinhamento_poscomp": "A questão está alinhada com o formato usualmente abordado em exames anteriores do POSCOMP (Exame Nacional para Ingresso na Pós-Graduação em Computação)?"
}

def generate_prompt(area, enunciado):
  dimension_prompts = "\n".join([f"- **{key}**: {value}" for key, value in dimensoes.items()])

  prompt = f"""## INSTRUÇÕES DE AVALIAÇÃO:
- Avalie a questão focando em seu enunciado e alternativas, ignorando textos adicionais.
- Para cada dimensão na lista abaixo, atribua uma nota de 1 a 3, seguindo a escala:
  - 1: Não (o critério não foi atendido)
  - 2: Mais ou menos (o critério foi parcialmente atendido)
  - 3: Sim (o critério foi totalmente atendido)
- Se a questão não contiver alternativas, todos os critérios relacionados a elas devem receber nota 1.
- Sua resposta deve ser estritamente um objeto JSON, sem nenhum texto adicional antes ou depois.

## DIMENSÕES DE AVALIAÇÃO:
{dimension_prompts}

## ÁREA DE GERAÇÃO:
{area}

## QUESTÃO PARA AVALIAÇÃO:
{enunciado}"""

  return prompt

mcqs = pd.read_csv('questoes_completo.csv')

mcqs.head(2)

for _, row in mcqs.iterrows():
  prompt = generate_prompt(f"{row['area']} - {row['subarea'] if not pd.Series(row['subarea']).isna().any() else ''}", row['answer_only'])

import ast

def evaluate_quality_with_gpt4(area, enunciado, model="gpt-4.1"):
    prompt = generate_prompt(area, enunciado)

    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "Você é um avaliador especialista em questões de múltipla escolha para o POSCOMP. Sua tarefa é avaliar a qualidade da questão fornecida com base em uma lista de critérios."},
            {"role": "user", "content": prompt}
        ],
        response_format={"type": "json_object"}
    )

    prompt_tokens = response.usage.prompt_tokens
    completion_tokens = response.usage.completion_tokens
    total_tokens = response.usage.total_tokens

    formatted_response = ast.literal_eval(response.choices[0].message.content)

    formatted_response.update({
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens
    })

    return formatted_response

import time

all_results = []
rate_limit = 28000
curr_tokens = 0

for index, question in mcqs.iterrows():
  print('Questão #', index)
  for iteration in range(5):
    area = f"{question['area']} - {question['subarea'] if not pd.Series(question['subarea']).isna().any() else ''}"
    formatted_question = question['answer_only']

    result = evaluate_quality_with_gpt4(area, formatted_question)

    curr_tokens += result['total_tokens']

    result.update({
      'id': question['id'],
      'model': question['model'],
      'version': question['version'],
      'iteration': iteration
    })

    all_results.append(result)

    if curr_tokens >= rate_limit:
      print('Descansando...')
      time.sleep(60)
      curr_tokens = 0

all_results_df = pd.DataFrame(all_results)

all_results_df.to_csv('all_results.csv', index = False)